---
title: "Chapter 4 Practice Exercises Solutions"
author: "Cuong Duong"
date: "2020-10-25"
output:
  html_notebook:
    default
---

```{r, eval=TRUE, results='hide'}
library(cmdstanr)
library(ggplot2)
library(data.table)
library(magrittr)
library(glue)

options(scipen = 999)
theme_set(theme_minimal())
```

### 4E1

#### Answer

$y_i \sim \mathrm{N(\mu, \sigma)}$ represents the likelihood.

### 4E2

#### Answer

There are two parameters in the posterior distribution: $\mu, \sigma$.

### 4E3

#### Answer

$\mathrm{Pr(\mu, \sigma |y)} = \dfrac{\mathrm{Pr(y |\mu\, \sigma)}\mathrm{Pr(\mu)Pr(\sigma)}}{\mathrm{Pr(y)}}$

Note that $\mu$ and $\sigma$ are independent.

### 4E4

#### Answer

$u_i = \alpha + \beta x_i$ represents the linear model.

### 4E5

#### Answer

There are three parameters in the posterior distribution: $\alpha, \mu, \sigma$.

### 4M1

#### Answer

```{r}
prior_samples <- list(
  mu = rnorm(n = 100000, mean = 0, sd = 10),
  sigma = rexp(n = 100000, rate = 1)
)

y_prior <- rnorm(100000, mean = prior_samples$mu, sd = prior_samples$sigma)
```

```{r}
hist(y_prior)
```
### 4M2

#### Answer

The following codes the model as a `stan` program.

```{r}
stan_model_simple <- "
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real mu;
  real sigma;
}
model {
  mu ~ normal(0, 10)
  sigma ~ exp(1)
  y ~ normal(mu, sigma)
}
"
```

### 4M3

#### Answer

$y_i \sim \mathrm{N(\mu_i, \sigma)}$

$\mu_i = \alpha + \beta x_i$

$\alpha \sim \mathrm{N(0, 10)}$

$\beta \sim \mathrm{U(0, 1)}$

$\sigma \sim \mathrm{Exp(1)}$

### 4M4

#### Answer

Note: We'll model with the un-standardized values of the height and year for ease of interpretation. Depending on the age of the students we're studying, we may have different prior knowledge about average heights and height growth over time (e.g. younger students vs. students at puberty).

```{r}
stan_model_height <- "
data {
  int<lower=0> N;
  vector[N] year;
  vector[N] height;
}
parameters {
  real alpha;
  real beta;
  real sigma;
}
transformed parameters {
  real mu = alpha + beta * year;
}
model {
  alpha ~ normal(150, 15);
  sigma ~ uniform(0, 30);
  beta ~ normal(0, 1);
  y ~ normal(mu, sigma);
}
"
```

### 4M5

#### Answer

If students get taller, the coefficient for `year` should always be positive. I would change the distribution of `beta` to be `gamma` or `lognormal` instead of `normal` to enforce positive values. The mean for the gamma distribution should also be more positive depending on the age of the students we're modelling; e.g. for students near puberty age, we would expect the coefficient to be more positive rather than close to zero.

### 4M6

#### Answer

We could truncate the `uniform` prior at 8 (`sqrt(64)`).

### 4M7

### Answer

```{r}
df <- fread("../data/howell_1.csv")
df_adult <- df[age >= 18, ]
```

Original model:

```{r}
m4_3_defn <- "
data {
  int<lower=0> N;
  vector<lower=0>[N] height;
  vector<lower=0>[N] weight;
  real weight_mean;
}
parameters {
  real<lower=0> sigma;
  real alpha;
  real<lower=0> beta;
}
transformed parameters {
  vector[N] mu;
  mu = alpha + beta * (weight - weight_mean);
}
model {
  height ~ normal(mu, sigma);
  alpha ~ normal(178, 20);
  beta ~ lognormal(0, 1);
  sigma ~ uniform(0, 100);
}
"
```

```{r, cache=TRUE, results='hide'}
m4_3 <- cmdstan_model(write_stan_file(m4_3_defn))
```

At the time of writing `cmdstanr` doesn't have the functionality to save the Hessian output from MAP optimization and draw samples from the quadratic approximation of the posterior, so we'll just use MCMC sampling instead.

```{r, results='hide'}
m4_3_fit <- m4_3$sample(
  data = c(as.list(df_adult), N = nrow(df_adult), weight_mean = mean(df_adult$weight)),
  seed = 123,
  adapt_delta = 0.95,
  iter_warmup = 10000,
  iter_sampling = 10000,
  parallel_chains = 4,
  refresh = 0,
  show_messages = FALSE
)
```

Now we re-specify the model without centering `weight`:

```{r}
m4_3_nocenter_defn <- "
data {
  int<lower=0> N;
  vector<lower=0>[N] height;
  vector<lower=0>[N] weight;
}
parameters {
  real<lower=0> sigma;
  real alpha;
  real<lower=0> beta;
}
transformed parameters {
  vector[N] mu;
  mu = alpha + beta * weight;
}
model {
  height ~ normal(mu, sigma);
  alpha ~ normal(178, 20);
  beta ~ lognormal(0, 1);
  sigma ~ uniform(0, 100);
}
"
```

```{r, cache=TRUE, results='hide'}
m4_3_nocenter <- cmdstan_model(write_stan_file(m4_3_nocenter_defn))
```

```{r, results='hide'}
m4_3_nocenter_fit <- m4_3_nocenter$sample(
  data = c(as.list(df_adult), N = nrow(df_adult)),
  seed = 123,
  adapt_delta = 0.95,
  max_treedepth = 12,
  iter_warmup = 10000,
  iter_sampling = 10000,
  parallel_chains = 4,
  refresh = 0,
  show_messages = FALSE
)
```

```{r}
m4_3_fit$summary(variables = c("sigma", "alpha", "beta"))
```

```{r}
m4_3_nocenter_fit$summary(variables = c("sigma", "alpha", "beta"))
```

Covariance among parameters:

```{r}
calculate_covariance <- function(stan_fit, variables) {
  cov_matrix <- stan_fit$draws(variables = variables) %>% 
    matrix(nrow = dim(.)[1] * dim(.)[2], dim(.)[3]) %>% 
    cov()
  colnames(cov_matrix) <- variables
  rownames(cov_matrix) <- variables
  
  cov_matrix
}
```

```{r}
calculate_covariance(m4_3_fit, c("sigma", "alpha", "beta")) %>% 
  round(4)
```

```{r}
calculate_covariance(m4_3_nocenter_fit, c("sigma", "alpha", "beta")) %>% 
  round(4)
```

The variance for `alpha` is much larger for the non-centered paramaterization. The covariance between `alpha` and the other parameters are also larger, whereas they are almost close to zero (which is our expectation as the parameters are independent) in the centered parameterization. 

So when we don't center, we can get more volatile results for the intercept of the linear model.

Now we compare the posterior predictions of the models:

```{r}
predict_height <- function(stan_fit, newdata, center = 0, posterior_bounds = c(0.05, 0.95)) {
  draws <- posterior::as_draws_matrix(stan_fit$draws(c("sigma", "alpha", "beta")))
  n_draws <- nrow(draws)
  n_obs <- nrow(newdata)
  
  preds <- matrix(0, nrow = n_obs, ncol = n_draws)
  for (i in seq_len(n_obs)) {
    mu <- draws[, "alpha"] + (newdata$weight[i] - center) * draws[, "beta"]
    preds[i, ] <- rnorm(length(mu), mu, draws[, "sigma"])
  }
  
  percentiles <- apply(preds, 1, quantile, probs = posterior_bounds, na.rm = TRUE)
  summary <- data.table(
    weight = newdata$weight,
    height_lower = percentiles[1, ],
    height_expected = rowMeans(preds, na.rm = TRUE),
    height_upper = percentiles[2, ]
  )
  
  summary
}
```

```{r}
height_preds_center <- predict_height(m4_3_fit, df_adult, center = mean(df_adult$weight))
height_preds_nocenter <- predict_height(m4_3_nocenter_fit, df_adult)
```

```{r}
plot_height_preds <- function(preds_df) {
  df <- copy(preds_df)
  df[, "obs_id" := .I]
  p <- df %>% 
    ggplot(aes(x = weight, y = height_expected, ymax = height_upper, ymin = height_lower)) +
    geom_point() +
    geom_ribbon(alpha = 0.4)
  
  p
}
```

```{r}
plot_height_preds(height_preds_center)
```

```{r}
plot_height_preds(height_preds_nocenter)
```

The model's height predictions and uncertainty are quite similar, however. Even though the variance of `beta` is higher for the non-centered model, the variance may 'cancel out' across the parameters, so the uncertainty of the predictions do not increase.

### 4M8

#### Answer

```{r}
dfc <- fread("../data/cherry_blossoms.csv")
dfc_doy <- dfc[complete.cases(doy), ]
```

```{r}
generate_basis_splines <- function(num_knots, degree) {
  knot_list <- quantile(dfc_doy$year, probs = seq(0, 1, length.out = num_knots))
  splines::bs(
    dfc_doy$year,
    knots = knot_list[-c(1, num_knots)] ,
    degree = degree, 
    intercept = TRUE
  )
}
```

```{r}
generate_spline_model_code <- function(weight_prior_sd) {
  model_code <- "
  data {
    int<lower=0> N;
    int<lower=0> num_weights;
    vector[N] doy;
    matrix[N, num_weights] B;
  }
  parameters {
    real<lower=0> sigma;
    real alpha;
    vector[num_weights] w;
  }
  transformed parameters {
    vector[N] mu;
    mu = alpha + B * w;
  }
  model {
    doy ~ normal(mu, sigma);
    alpha ~ normal(100, 10);
    w ~ normal(0, {{weight_prior_sd}});
    sigma ~ exponential(1);
  }
  "
  model_code <- glue(model_code, weight_prior_sd = weight_prior_sd, .open = "{{", .close = "}}")
  model_code
}
```

We will test the following scenarios:

* `num_knots`: 15, 30
* `weight_prior_sd`: 1, 10, 100

```{r}
basis_splines <- list(
  lower = generate_basis_splines(15, 3),
  higher = generate_basis_splines(50, 3)
)
```

```{r}
models_code <- list(
  narrow = generate_spline_model_code(weight_prior_sd = 1),
  medium = generate_spline_model_code(weight_prior_sd = 10),
  wide = generate_spline_model_code(weight_prior_sd = 100)
)
```

```{r, results='hide'}
models <- lapply(models_code, function(x) cmdstan_model(write_stan_file(x)))
```

```{r}
fit_all_models <- function(models, basis_splines) {
  models_cb_fit <- list() 
  for (i in seq_along(basis_splines)) {
    spline_type <- names(basis_splines)[i]
    for (j in seq_along(models)) {
      model_type <- names(models)[j]
      models_cb_fit[[glue("spline:{spline_type}--weight_sd:{model_type}")]] <- models[[j]]$optimize(
        data = list(doy = dfc_doy$doy, N = nrow(dfc_doy), num_weights = ncol(basis_splines[[i]]), B = basis_splines[[i]]),
        seed = 123,
        algorithm = "lbfgs",
        refresh = 0,
        init = list(
          list(w = 0)
        )
      )
    }
  }
  
  models_cb_fit
}
```

```{r, results='hide'}
models_cb_fit <- fit_all_models(models, basis_splines)
```

```{r}
get_predictions <- function(spline_model) {
  preds <- spline_model$mle("mu")
  preds_df <- data.table(year = dfc_doy$year, doy = dfc_doy$doy, doy_pred = preds)
  
  preds_df
}
compare_predictions <- function(models_list) {
  all_preds <- lapply(models_list, get_predictions)
  all_preds_df <- rbindlist(all_preds, idcol = "model")
  all_preds_df[, "model" := factor(model, names(models_list))]
  all_preds_df
}
```

```{r}
all_preds <- compare_predictions(models_cb_fit)
```

```{r}
all_preds %>% 
  ggplot(aes(x = year, y = doy_pred)) +
  geom_point(aes(x = year, y = doy), alpha = 0.2) +
  geom_line(colour = "darkred", size = 1) +
  facet_wrap(~model, ncol = 3)
```
A narrow prior effectively penalizes the fitting process and ensures the fitted line isn't too "wiggly". This is because a narrow prior requires more data to move the mean of `w` away from zero. We can see that the model with the wide standard deviation prior produces a more wiggly fit. Increasing the number of knots has a similar effect.

### 4H1

#### Answer

We'll use the centered height model from exercise 4M7.

```{r}
predict_height(
  m4_3_fit, 
  data.table(weight = c(46.95, 43.72, 64.78, 32.59, 54.63)), 
  center = mean(df_adult$weight),
  posterior_bounds = c(0.055, 1 - 0.055)
)
```

### 4H2

#### Answer

```{r}
df_child <- df[age < 18, ]
```

(a)

```{r}
m_height_child_defn <- "
data {
  int<lower=0> N;
  vector<lower=0>[N] height;
  vector<lower=0>[N] weight;
}
parameters {
  real<lower=0> alpha;
  real<lower=0> sigma;
  real beta;
}
transformed parameters {
  vector[N] mu = alpha + beta * weight;
}
model {
  height ~ normal(mu, sigma);
  alpha ~ normal(130, 20);
  beta ~ normal(0, 1);
  sigma ~ exponential(1);
}
"
```

```{r, results='hide'}
m_height_child <- cmdstan_model(write_stan_file(m_height_child_defn))
```

```{r, results='hide'}
m_height_child_fit <- m_height_child$sample(
  data = c(as.list(df_child), N = nrow(df_child)),
  seed = 123,
  adapt_delta = 0.95,
  iter_warmup = 10000,
  iter_sampling = 10000,
  parallel_chains = 4,
  refresh = 0,
  show_messages = FALSE
)
```

The main parameter estimate from the model is `beta`, the relationship between height and weight. We plot its marginal posterior distribution below.

```{r}
posterior::as_draws_df(m_height_child_fit$draws("beta")) %>% 
  ggplot(aes(x = beta)) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(xintercept = m_height_child_fit$summary("beta")[["q5"]]) +
  geom_vline(xintercept = m_height_child_fit$summary("beta")[["q95"]])
```

The 90% percentile interval is approximately [2.59, 2.8], so for every 10 units of increase in weight we expect 25.9 to 28 units increase in height.

(b)

For each child, calculate the posterior mean of `mu` and the 89% percentile interval.

```{r}
summarise_mu_height <- function(fit, data, bounds) {
  posterior_mu <- posterior::as_draws_matrix(fit$draws("mu"))
  bounds_mu <- apply(posterior_mu, 2, quantile, bounds)
  
  mu_summary <- data.table(
    weight = data$weight,
    height = data$height,
    lower = bounds_mu[1, ],
    mean = apply(posterior_mu, 2, mean),
    upper = bounds_mu[2, ]
  )
  
  mu_summary
}

summarise_predictions_height <- function(fit, data, bounds) {
  posterior_mu <- posterior::as_draws_matrix(fit$draws("mu"))
  posterior_sigma <- posterior::as_draws_matrix(fit$draws("sigma"))
  preds <- matrix(0, nrow = nrow(posterior_mu), ncol = nrow(data))
  for (i in seq_len(nrow(data))) {
    preds[, i] <- rnorm(nrow(posterior_mu), posterior_mu[, i], posterior_sigma[, "sigma"])
  }
  
  bounds_preds <- apply(preds, 2, quantile, bounds)
  preds_summary <- data.table(
    weight = data$weight,
    height = data$height,
    height_lower = bounds_preds[1, ],
    height_expected = apply(preds, 2, mean),
    height_upper = bounds_preds[2, ]
  )
  
  preds_summary
}
```

```{r}
plot_m_height <- function(mu_summary, preds_summary, title) {
  p <- ggplot() +
    geom_point(data = mu_summary, aes(x = weight, y = height), alpha = 0.2) +
    geom_line(data = mu_summary, aes(x = weight, y = mean), size = 0.8, colour = "darkred") +
    geom_ribbon(data = mu_summary, aes(x = weight, ymin = lower, ymax = upper), fill = "red", alpha = 0.3) +
    geom_ribbon(data = preds_summary, aes(x = weight, ymin = height_lower, ymax = height_upper), fill = "blue", alpha = 0.2) +
    labs(title = title)

  p
}
```

```{r}
plot_m_height(
  summarise_mu_height(m_height_child_fit, df_child, c(0.055, 1 - 0.055)),
  summarise_predictions_height(m_height_child_fit, df_child, c(0.055, 1 - 0.055)),
  "Child Height Model"
)
```

(c) The model doesn't fit the weight of heavier children well. It assumes a linear relationship between weight and height, but the relationship is not linear as weight grows (i.e. heavier children are less likely to have proportionate increases in height). I would try to fit a polynomial regression (even quadratic might fit this dataset well) instead.

### 4H3

#### Answer

(a)

```{r}
m_height_log_defn <- "
data {
  int<lower=0> N;
  vector<lower=0>[N] height;
  vector<lower=0>[N] weight;
}
parameters {
  real<lower=0> alpha;
  real<lower=0> sigma;
  real beta;
}
transformed parameters {
  vector[N] mu = alpha + beta * log(weight);
}
model {
  height ~ normal(mu, sigma);
  alpha ~ normal(130, 20);
  beta ~ normal(0, 1);
  sigma ~ exponential(1);
}
"
```

```{r, results='hide'}
m_height_log <- cmdstan_model(write_stan_file(m_height_log_defn))
```

```{r, results='hide'}
m_height_log_fit <- m_height_log$sample(
  data = c(as.list(df), N = nrow(df)),
  adapt_delta = 0.9,
  seed = 123, 
  refresh = 0, 
  chains = 4, 
  parallel_chains = 4, 
  iter_warmup = 10000, 
  iter_sampling = 10000,
  show_messages = FALSE
)
```

```{r}
m_height_log_fit$summary(variables = c("alpha", "beta", "sigma"))
```

The parameter of interest here is `beta`. It represents the increase in height for every unit increase in `log(weight)`. This is estimated to be 16.46, with a range of [14.31, 18.622]. For lower values of `weight`, the log increase will match more closely with the raw increase in `weight`, but people get heavier, the model is saying there must be a relatively large increase in `weight` before `height` increases.

(b)

```{r}
plot_m_height(
  summarise_mu_height(m_height_log_fit, df, bounds = c(0.015, 1 - 0.015)),
  summarise_predictions_height(m_height_log_fit, df, bounds = c(0.015, 1 - 0.015)),
  "General Height Model - Log Weight Predictor"
)
```
The fit is not great: the logarithm curve does not increase steeply enough with `weight`. Because of this, the lower tails are over-predicted and the upper tails are under-predicted.

### 4H4

#### Answer

The model is:

$h_i \sim \mathrm{N(\mu_i, \sigma)}$

$mu_i = \alpha + \beta_1 x_i + \beta_2 x_{i}^2$

$\alpha \sim N(178, 20)$

$\beta_1 \sim \mathrm{LN(0, 1)}$

$\beta_2 \sim \mathrm{N(0, 1)}$

$\sigma \sim \mathrm{U(0, 50)}$

where $x_i$ is the standardized weight.

We simulate the prior predictive distribution for each `weight` value.

```{r}
sample_prior_predictive <- function(weight_vector, num_samples, params) {
  alpha <- rnorm(num_samples, params$alpha['mean'], params$alpha['sd'])
  beta1 <- rlnorm(num_samples, params$beta1['mean'], params$beta2['sd'])
  beta2 <- rnorm(num_samples, params$beta2['mean'], params$beta2['sd'])
  sigma <- runif(num_samples, params$sigma['lower'], params$sigma['upper'])
  weight_std <- (weight_vector - mean(weight_vector)) / sd(weight_vector)
    
  preds <- matrix(0, nrow = length(weight_vector), ncol = num_samples)
  for (i in seq_along(weight_vector)) {
    mu <- alpha + beta1 * weight_std[i] + beta2 * weight_std[i]^2
    preds[i, ] <- rnorm(num_samples, mu, sigma)
  }
  
  preds
}

summarise_prior_predictive <- function(weight_vector, preds, bounds = c(0.05, 0.95)) {
  percentiles <- apply(preds, 1, quantile, probs = bounds)
  df <- data.table(
    weight = weight_vector,
    height_lower = percentiles[1, ],
    height_expected = apply(preds, 1, mean),
    height_upper = percentiles[2, ]
  )
  
  df
}

plot_prior_predictive <- function(summary) {
  p <- ggplot(data = summary, aes(x = weight, y = height_expected,  ymin = height_lower, ymax = height_upper)) +
    geom_line(colour = "darkred", size = 0.8) +
    geom_ribbon(fill = "red", alpha = 0.2)
  
  p
}

test_prior_params <- function(params) {
  preds <- sample_prior_predictive(weight_vector = df$weight, num_samples = 100000, params)
  preds_summary <- summarise_prior_predictive(df$weight, preds)
  
  plot_prior_predictive(preds_summary)
}
```

```{r}
test_prior_params(
  list(
    alpha = c(mean = 178, sd = 20),
    beta1 = c(mean = 0, sd = 1),
    beta2 = c(mean = 0, sd = 1),
    sigma = c(lower = 0, upper = 50)
  )
) 
```
Some things we could try fixing:
* It's unlikely that people with weights < 20 (children) will be above 175 in height. `alpha` likely needs to have a lower mean.
* The real slope of the line is likely steeper. The current slope increases very slowly with weight.
* As a result of the point 2), the line doesn't extend far enough to reach taller heights. It caps at around 180, but heights of ~190 are plausible for weights around 65-70.

```{r}
test_prior_params(
  list(
    alpha = c(mean = 150, sd = 20),
    beta1 = c(mean = 2, sd = 1),
    beta2 = c(mean = 0, sd = 1),
    sigma = c(lower = 0, upper = 50)
  )
) 
```

### 4H5

#### Answer

```{r}
dfc_doy_temp <- dfc[complete.cases(dfc[, .(temp, doy)])]
```

We'll plot the relationship between `temp` and `doy` first to get an idea of which model we want to use.

```{r}
ggplot(dfc_doy_temp, aes(x = temp, y = doy)) +
  geom_point()
```

It's hard to see a univariate model that does well given this scatterplot. There's no clear relationship between the March temperature and the blossom date. There is a slight negative trend (higher temperature -> blossoms earlier) but even so it is weak.

The most effective model to consider here is probably a linear model.

```{r}
m_cb_temp_defn <- "
data {
  int<lower=0> N;
  vector<lower=0>[N] doy;
  vector[N] temp;
  real mean_temp;
}
parameters {
  real<lower=0> sigma;
  real beta;
  real<lower=0> alpha;
}
transformed parameters {
  vector[N] mu = alpha + beta * (temp - mean_temp);
}
model {
  doy ~ normal(mu, sigma);
  alpha ~ normal(100, 20);
  beta ~ normal(0, 1);
  sigma ~ exponential(1);
}
"
```

```{r, results='hide'}
m_cb_temp <- cmdstan_model(write_stan_file(m_cb_temp_defn))
```

```{r, results='hide'}
m_cb_temp_fit <- m_cb_temp$optimize(
  data = c(as.list(dfc_doy_temp), N = nrow(dfc_doy_temp), mean_temp = mean(dfc_doy_temp$temp)),
  seed = 123, 
  refresh = 0,
  algorithm = "lbfgs"
)
```

```{r}
dfc_doy_temp[, "doy_expected" := m_cb_temp_fit$mle("mu")]
```

```{r}
dfc_doy_temp %>% 
  ggplot(aes(x = temp)) +
  geom_point(aes(y = doy), alpha = 0.2) +
  geom_line(aes(y = doy_expected), colour = "darkred", size = 0.8)
```

As expected, the linear model is showing a negative relationship between March temperature and the blossom date; the higher the temperature, the earlier the blossom date. However, the model fit is not good, as there are many dates where the blossom date is much later or sooner than the predicted date for a given temperature level.

### 4H6

#### Answer

```{r}
simulate_prior_predictive_doy <- function(year_vector, num_samples = 100000, params) {
  B <- generate_basis_splines(15, 3)
  sigma <- rexp(num_samples, params$sigma['rate'])
  alpha <- rnorm(num_samples, params$alpha['mean'], params$alpha['sd'])
  weights <- matrix(0, nrow = num_samples, ncol = ncol(B))
  for (i in seq_len(ncol(B))) {
    weights[, i] <- rnorm(num_samples, params$w['mean'], params$w['sd'])
  }
  
  preds <- matrix(0, nrow = length(year_vector), ncol = num_samples)
  for (i in seq_along(year_vector)) {
    mu <- alpha + weights %*% B[i, ]
    preds[i, ] <- rnorm(num_samples, mu, sigma)
  }
  
  preds
}

summarise_prior_predictive_doy <- function(year_vector, preds, bounds = c(0.05, 0.95)) {
  percentiles <- apply(preds, 1, quantile, bounds)
  df <- data.table(
    year = year_vector,
    doy_lower = percentiles[1, ],
    doy_expected = apply(preds, 1, mean),
    doy_upper = percentiles[2, ]
  )
  
  df
}

plot_prior_predictive_doy <- function(summary) {
  p <- ggplot(summary, aes(x = year)) +
    geom_line(aes(y = doy_expected), colour = "darkred", size = 0.8) +
    geom_ribbon(aes(ymin = doy_lower, ymax = doy_upper), fill = "red", alpha = 0.2)
  
  p
}

test_prior_predictive_doy <- function(params) {
  preds <- simulate_prior_predictive_doy(dfc_doy$year, 100000, params)
  preds_summary <- summarise_prior_predictive_doy(dfc_doy$year, preds)
  
  plot_prior_predictive_doy(preds_summary)
}
```

Simulating the priors proposed in the chapter:

```{r}
test_prior_predictive_doy(
  list(
    alpha = c(mean = 100, sd = 10),
    sigma = c(rate = 1),
    w = c(mean = 0, sd = 10)
  )
) +
  labs(title = "sd on weight prior = 10")
```

Testing a lower standard deviation on the `w` prior:

```{r}
test_prior_predictive_doy(
  list(
    alpha = c(mean = 100, sd = 10),
    sigma = c(rate = 1),
    w = c(mean = 0, sd = 1)
  )
) +
  labs(title = "sd on weight prior = 1")
```

Versus a higher standard deviation for the `w` prior:

```{r}
test_prior_predictive_doy(
  list(
    alpha = c(mean = 100, sd = 10),
    sigma = c(rate = 1),
    w = c(mean = 0, sd = 50)
  )
) +
  labs(title = "sd on weight prior = 50")
```

When the standard deviation of `w` is lower, the range of plausible values for `doy` as suggested by the prior is much smaller. Conversely, a larger standard deviation expands the range of values, especially at the ends of the fit (the earliest and latest years).

The weights are multipled by the basis functions to get `mu`, so if we have a lower standard deviation on the `w` prior, we are keeping our estimates of `w` closer to 0 unless the data strongly suggests otherwise. This effectively acts as a "penalty" on the weights.

### 4H8

#### Answer

```{r}
num_knots <- 15
degree <- 3
knot_list <- quantile(dfc_doy$year, probs = seq(0, 1, length.out = num_knots))
B <- splines::bs(
  dfc_doy$year,
  knots = knot_list[-c(1, num_knots)] ,
  degree = degree, 
  intercept = TRUE
)
```

```{r}
m_doy_nointercept_defn <- "
data {
  int<lower=0> N;
  int<lower=0> num_weights;
  vector[N] doy;
  matrix[N, num_weights] B;
}
parameters {
  real<lower=0> sigma;
  vector[num_weights] w;
}
transformed parameters {
  vector[N] mu;
  mu = B * w;
}
model {
  (doy - mean(doy)) ~ normal(mu, sigma);
  w ~ normal(0, 10);
  sigma ~ exponential(1);
}
"
```

```{r, results='hide'}
m_doy_nointercept <- cmdstan_model(write_stan_file(m_doy_nointercept_defn))
```

```{r}
m_doy_nointercept_fit <- m_doy_nointercept$optimize(
  data = list(doy = dfc_doy$doy, num_weights = ncol(B), B = B),
  seed = 123, 
  refresh = 0,
  algorithm = "lbfgs"
)
```

The fit fails here.

